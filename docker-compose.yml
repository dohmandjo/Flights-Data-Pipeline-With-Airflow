version: "3.8"

services:
  # Kafka (KRaft, single broker)
  kafka:
    image: bitnami/kafka:3.7
    container_name: kafka
    ports:
      - "9091:9091"  # host-visible client port (optional)
    environment:
      - KAFKA_ENABLE_KRAFT=yes
      - KAFKA_CFG_NODE_ID=1
      - KAFKA_CFG_PROCESS_ROLES=controller,broker
      - KAFKA_CFG_CONTROLLER_QUORUM_VOTERS=1@kafka:9093
      # Three listeners: internal broker (9092), controller (9093), host client (9094)
      - KAFKA_CFG_LISTENERS=PLAINTEXT://:9092,CONTROLLER://:9093,PLAINTEXT_HOST://:9091
      - KAFKA_CFG_ADVERTISED_LISTENERS=PLAINTEXT://kafka:9092,PLAINTEXT_HOST://localhost:9091
      - KAFKA_CFG_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,CONTROLLER:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_CFG_CONTROLLER_LISTENER_NAMES=CONTROLLER
      - KAFKA_CFG_INTER_BROKER_LISTENER_NAME=PLAINTEXT
      - KAFKA_CFG_AUTO_CREATE_TOPICS_ENABLE=true
      - ALLOW_PLAINTEXT_LISTENER=yes
    healthcheck:
      test: ["CMD-SHELL", "/opt/bitnami/kafka/bin/kafka-topics.sh --bootstrap-server localhost:9092 --list || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 10

  # Python producer
  producer:
    build:
      context: ./kafkaproducer
      dockerfile: Dockerfile
    container_name: producer_flight
    environment:
      - AVIATIONSTACK_KEY=${AVIATIONSTACK_KEY}     # put this in your .env
      - BOOTSTRAP_SERVERS=kafka:9092               # talks to the internal listener
      - TOPIC=flights_live
    depends_on:
      kafka:
        condition: service_healthy
    restart: unless-stopped

  # Postgres (unchanged; keep if you need it)
  postgres:
    image: postgres:16
    container_name: postgres_flight
    ports:
      - "5433:5432"
    environment:
      - POSTGRES_USER=${PGUSER}
      - POSTGRES_PASSWORD=${PGPASSWORD}
      - POSTGRES_DB=${PGDATABASE}
    volumes:
      - pgdata:/var/lib/postgresql/data
      - ./postgres:/docker-entrypoint-initdb.d:ro
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${PGUSER} -d ${PGDATABASE} -p 5432"]
      interval: 5s
      timeout: 5s
      retries: 20

  spark:
    build: ./spark_app
    container_name: spark_flight
    environment:
      - KAFKA_BOOTSTRAP=kafka:9092
      - KAFKA_TOPIC=flights_live
      - PGHOST=${PGHOST}    #flight_postgres
      - PGPORT=${PGPORT}  ##5432
      - PGDATABASE=${PGDATABASE}
      - PGUSER=${PGUSER}
      - PGPASSWORD=${PGPASSWORD}
    depends_on:
      kafka:
        condition: service_healthy
      postgres:
        condition: service_healthy
    command:
      [
        "/opt/bitnami/spark/bin/spark-submit",
        "--conf","spark.jars.ivy=/opt/bitnami/spark/app/ivy",
#        "--repositories","https://repo1.maven.org/maven2",
        "--packages","org.postgresql:postgresql:42.7.3,org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.6",
        "/opt/bitnami/spark/app/stream_to_postgres.py"
      ]

  airflow:
    build: ./airflow
    image: my-airflow:2.9.3-postgres
    container_name: airflow_flight
    environment:
      - AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True
      - AIRFLOW__SCHEDULER__PARSING_PROCESSES=1
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - AIRFLOW__CORE__EXECUTOR=LocalExecutor
      - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres:5432/airflow
      - AIRFLOW__CORE__FERNET_KEY=your_32byte_base64_fernet_key_here
      - AIRFLOW__WEBSERVER__SECRET_KEY=some_random_webserver_secret
      - AIRFLOW_UID=50000
      - AIRFLOW_CONN_WAREHOUSE_DB=postgresql+psycopg2://flight_user:Password123@postgres:5432/flight_pipeline

      # Warehouse connection details
      - PGHOST=${PGHOST}
      - PGPORT=${PGPORT}
      - PGDATABASE=${PGDATABASE}
      - PGUSER=${PGUSER}
      - PGPASSWORD=${PGPASSWORD}
      #Google sheets
      - GSHEET_ID=${GSHEET_ID}
      - GWORKSHEET_TITLE=${GWORKSHEET_TITLE}
      - GOOGLE_APPLICATION_CREDENTIALS=/secrets/gsa-keys.json

    ports:
      - "8080:8080"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - ./gsheet_sync/elated-badge-471323-q5-020b3e2276e0.json:/secrets/gsa-keys.json:ro
      # FIX this path: you mounted ./producer earlier, but your producer folder is ./kafkaproducer
      # remove or correct as needed; Airflow doesn't need it unless your DAG imports it
      # - ./kafkaproducer:/opt/airflow/kafkaproducer:ro
    command: >
      bash -c "
        airflow db migrate &&
        pip install --no-cache-dir pandas SQLAlchemy psycopg2-binary gspread gspread-dataframe &&
        airflow users create --username admin --password admin --firstname a --lastname b --role Admin --email admin@example.com || true &&
        airflow webserver -p 8080 &
        airflow scheduler
      "
    depends_on:
      postgres:
        condition: service_healthy
      kafka:
        condition: service_healthy
    restart: unless-stopped

  gsheet_sync:
    build:
      context: ./gsheet_sync
      dockerfile: Dockerfile  # a small python image with your script
    container_name: gsheet_flight
    environment:
      PGHOST: ${PGHOST} #flight_postgres
      PGPORT: ${PGPORT} #"5432"
      PGDATABASE: ${PGDATABASE} #flight_pipeline
      PGUSER: ${PGUSER} #flight_user
      PGPASSWORD: ${PGPASSWORD} #P@ssword123
      GOOGLE_APPLICATION_CREDENTIALS: /secrets/gsa-keys.json #elated-badge-471323-q5.json
      GSHEET_ID: ${GSHEET_ID} #Flights Warehouse
      GWORKSHEET_TITLE: ${GWORKSHEET_TITLE} #Fact_Status
    volumes:
      - ./gsheet_sync/pg_to_gsheet.py:/app/pg_to_gsheet.py:ro
      - ./gsheet_sync/elated-badge-471323-q5-020b3e2276e0.json:/secrets/gsa-keys.json:ro
    depends_on:
      postgres:
        condition: service_healthy
    command: [ "bash", "-lc", "while true; do python /app/pg_to_gsheet.py; sleep 10; done" ]

volumes:
  pgdata:
